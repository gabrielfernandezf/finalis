# -*- coding: utf-8 -*-
"""Finalis_Challenge_VF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tIe1Ss0tOKj2gFDtQeAQdMikjkgMyZXN

# ***Project Documentation: Exploring Birth Weight Data***
---


---




## **Overview**:


---


Welcome to the exploration of birth weight data! This project aims to analyze factors influencing birth weight and understand their impact on newborns' health. The dataset contains information about parents' characteristics and prenatal care, providing valuable insights into potential predictors of birth weight.

## **Sections Covered:**


---




1.   **Data Cleaning and Preprocessing:**
- Handled missing values using mean imputation.
- Created a new feature 'smoking' based on maternal smoking habits.
- Scaled birth weight from grams to kilograms for better interpretation.
- Managed outliers, such as setting an age limit for mothers.

2. **Exploratory Data Analysis (EDA):**

- Conducted descriptive analysis to understand the data distribution.
- Examined correlations between variables using a heatmap.
- Visualized numerical variables' distributions with histograms.

3. **Statistical Testing:**

- Utilized statistical tests to compare birth weights between smoking and - non-smoking mothers.
- Interpreted results to determine significant differences in birth weights.

4. **Predictive Modeling:**

- Implemented Random Forest Regressor for birth weight prediction.
- Tuned hyperparameters using GridSearchCV for optimal model performance.
- Evaluated model using metrics like Mean Squared Error and R-squared.

5. **Feature Engineering & Selection:**

- Identified important features using Recursive Feature Elimination.
- Addressed multicollinearity through VIF analysis and feature selection.

6. **Model Interpretation with SHAP:**

- Employed SHAP values to interpret the Random Forest model's predictions.
- Visualized feature importance for better understanding of model decisions.

7. **Cross-Validation & Model Evaluation:**

- Conducted 5-fold cross-validation to assess model performance.
- Calculated Mean Squared Error for each fold and overall mean.

## **Hey! Before you start reviewing the notebook:**
### In this repo you can find a docker file and .py files for deploy. This is an example of my approach for productive code using OOP.

# INSTALLING DEPENDENCIES, IMPORTING LIBRARIES AND LOADING DATA
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install missingno streamlit skimpy pycaret shap
# 
# !pip install scikit-learn --upgrade

"""### DATA DICTIONARY


| variable | label  | description                  |
|----------|--------|------------------------------|
| 1        | mage   | mother's age                 |
| 2        | meduc  | mother's education           |
| 3        | monpre | month prenatal care began    |
| 4        | npvis  | total number of prenatal visits |
| 5        | fage   | father's age, years          |
| 6        | feduc  | father's education, years    |
| 7        | omaps  | one minute Apgar score       |
| 8        | fmaps  | five minute Apgar score      |
| 9        | cigs   | average cigarettes per day   |
| 10       | drink  | average drinks per week      |
| 11       | male   | 1 if baby male               |
| 12       | mwhte  | 1 if mother white            |
| 13       | mblck  | 1 if mother black            |
| 14       | moth   | 1 if mother is other         |
| 15       | fwhte  | 1 if father white            |
| 16       | fblck  | 1 if father black            |
| 17       | foth   | 1 if father is other         |
| 18       | bwght  | birthweight, grams           |

"""

import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns
from skimpy import skim

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import scipy.stats as stats
from pycaret.regression import *

from sklearn.ensemble import RandomForestRegressor
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.feature_selection import RFECV
from sklearn.linear_model import LinearRegression

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import shap

from sklearn.model_selection import cross_val_score

df = pd.read_excel('birthweight_low.xlsx')

pd.set_option('display.max_rows', None)

"""# Exercise 1: Data Cleaning and Preprocessing"""

df.head()

df.info()

df.isnull().sum()

msno.matrix(df)

skim(df)

df.hist(figsize=(12, 10))
plt.tight_layout()
plt.show()

columnas = df.columns
columnas_por_fila = 4

num_filas = (len(columnas) + columnas_por_fila - 1) // columnas_por_fila

plt.figure(figsize=(15, 5 * num_filas))
for i, columna in enumerate(columnas):

    fila = i // columnas_por_fila
    columna_actual = i % columnas_por_fila + 1
    plt.subplot(num_filas, columnas_por_fila, i + 1)
    plt.boxplot(df[columna].astype(float))
    plt.xlabel(columna)
    plt.ylabel('VAlue')
    plt.title(f'{columna}s Boxplot')

plt.tight_layout()
plt.show()

"""### In the context of imputing ages of mothers older than 55 years in the birth weight dataset, the criterion used to select the imputation with a specific value (in this case, 55 years) is based on the following logic:

- Age Limit for Gestation: Since the mean age limit for gestation based on the average age of menopause occurrence (45-55 years), imputing the ages of mothers older than 55 years with 55 years is consistent with this limit.

- Consistency and Simplification: By imputing the ages of mothers older than 55 years with a fixed value of 55 years, data consistency is maintained, and the imputation process is simplified by assigning a value that reflects the established limit.

- Preservation of Distribution: Imputing with a specific value like 55 years avoids distorting the original distribution of ages and ensures that the data remains representative within the established range.

- Ease of Interpretation: Using a specific value like 55 years to impute the ages of mothers older than 55 years facilitates data interpretation and simplifies the subsequent analysis process.

### MISSING VALUES, OUTLIERS & SCALING TREATMENT
"""

imputer = SimpleImputer(strategy='mean') #Easy and consistent imputation due to data length and simplicity
df[['meduc', 'npvis', 'feduc']] = imputer.fit_transform(df[['meduc', 'npvis', 'feduc']])

# New Feature 'smoking_status'
df['smoking'] = df['cigs'].apply(lambda x: 0 if x == 0 else 1)

# Grams to Kilograms (Scaling)
df['bwght_kg'] = (df['bwght'] / 1000).round(2)

# Outliers - Moms Age not greater than 55 (menopause)
df.loc[df['mage'] > 55, 'mage'] = 55

df_final = df.copy().drop("bwght", axis = 1)

"""# Exercise 2: Exploratory Data Analysis (EDA)

### This helps to check the NaN imputation, lack of outliers and data consistency. Is a data validation instance after a few transformations.
"""

df.describe().round(2).T

# Direction and how strong are the variables related
Ticorr_matrix = df_final.corr()
plt.figure(figsize=(20, 15))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlaci√≥n')
plt.show()

df_final.hist(figsize=(12, 10))
plt.tight_layout()
plt.show()

"""- Histograms are used to visually understand the distribution of data, given their statistical nature when dealing with discrete variables. They help identify skewness, dispersion/variability, provide an approximation to detect anomalies, trends, and the presence of outliers.

# Exercise 3: Statistical Testing
"""

plt.figure(figsize=(10, 8))
sns.violinplot(x='smoking', y='bwght_kg', data=df_final)
plt.title('Smoking influence over Birth weight')
plt.show()

"""### Conclusions:


- Based on the sample, we can say that the distribution of birth weights for babies, in the case of non-smoking mothers, skews towards the left but shows mean values above 3 kilograms and reaching values exceeding 5 kilograms. Additionally, in terms of dispersion, we observe a relatively tighter concentration in this distribution compared to that of smoking mothers. As for smoking mothers, we see a right-skewed distribution and greater dispersion towards the tails, with values even below one kilogram. The extended tail indicates the dispersion of these data, leading to the conclusion that smoking negatively affects the weight of babies, unlike non-smoking mothers where the weight tends to be over 2 kilograms in this sample.
Here we observe mean values, dispersion, and skewness of the data to visually draw some conclusions.

- In the next step we validate this observations with a t-statistic test to prove statistic significance
"""

smoked_bw = df_final[df_final['smoking'] == 1]['bwght_kg']
non_smoked_bw = df_final[df_final['smoking'] == 0]['bwght_kg']


t_statistic, p_value = stats.ttest_ind(smoked_bw, non_smoked_bw, equal_var=False) # t-test


print("t-statistic:", t_statistic) # Results
print("p-value:", p_value)

"""### Result: There is a statistically significant difference in birth weight between babies born to mothers who smoked during pregnancy and those who did not."""

df_final.head()

"""# Pre Exercise 4: Predictive Modeling using PyCaret approach

#### Each time I finish a data preprocessing stage, I use PyCaret to start the model selection process. PyCaret is a low-code library that allows us to run multiple models at once, providing a summary of metrics to compare performances, ranking the models based on metric scores. Then, we can select the best-performing model for hyperparameter tuning or make predictions with a trained model, which also includes a feature selection stage based on feature importance analysis.
### Additionally, it provides a summary of all the steps and characteristics of the model. This can be useful for quickly approaching a potential model that could be an excellent fit for our data and then continue developing from there.
"""

# PyCaret Config
exp_reg = setup(data=df_final, target='bwght_kg', session_id=123)

best_model = compare_models()

"""###Interpretation:
- The Extra Trees Regressor (et) model has the lowest MAE and MSE, indicating the lowest average error in predicting birth weight.
The Extreme Gradient Boosting (xgboost) model performs well with a high R2 and low RMSLE.
- Both Gradient Boosting Regressor (gbr) and Random Forest Regressor (rf) show similar results in terms of evaluation metrics.
- These results allow you to compare the performance of the top 4 models based on different evaluation metrics and select the most suitable one for your analysis based on the metrics you consider most important for your specific case.
"""

evaluate_model(best_model)

final_model = finalize_model(best_model)

"""# Exercise 4: Predictive Modeling using Random Forest Regressor

###Both XGBoost and Random Forest Regressor are popular and powerful machine learning models that often yield good results in a variety of regression problems. Here are some considerations to help you decide between XGBoost and Random Forest Regressor:

###XGBoost:
- Advantages: Tends to be faster and more efficient in terms of training time compared to Random Forest, especially with large datasets.
Can easily handle non-linearity and feature interactions.
Less prone to overfitting due to its regularization capability.
Proven effective in data science competitions and widely used in the industry.

- Limitations: Requires careful hyperparameter tuning to achieve optimal performance.
May be more sensitive to noise and outliers compared to Random Forest.

###Random Forest Regressor:
- Advantages: Robust against outliers and noisy data due to the combination of multiple decision trees.
Less prone to overfitting compared to individual decision trees.
Does not require extensive hyperparameter tuning and is easy to use.

- Limitations:
Can be slower in terms of training time compared to XGBoost, especially with large datasets.
May have slightly lower performance on very large and complex datasets.

###Model Choice:
- Based on the results provided, both models (XGBoost and Random Forest) have similar performance in terms of evaluation metrics.
- We will choose a Random Forest Regressor model in this case due to the advantages it offers in terms of simplicity for configuring its hyperparameters, ease of explanation to non-technical individuals, and also considering that we are working with a relatively small dataset, which allows me to meet the challenge requirements.
- This decision may change if we later work with larger datasets or if the model becomes more complex in terms of variables to use, deployment considerations, and other factors that affect the performance during model deployment and real-time consumption.

## Feature Engineering & Selection

### Random Forest Regressor for feature importances
"""

rf = RandomForestRegressor()
rf.fit(df_final.drop('bwght_kg', axis=1), df_final['bwght_kg'])
importances_rf = rf.feature_importances_

importances_series = pd.Series(importances_rf.flatten())

variable_names = df_final.columns[:-1].to_list()

df_importances = pd.DataFrame({'Variable': variable_names, 'Importance': importances_series})

df_importances

"""- Notes : Many times, we encounter a few features with lower importance rates concerning the target variables. Deciding whether to keep or drop these variables will be related to qualitative knowledge about the problem.

- At times, it is beneficial to continue working with all the columns and not to drop them despite their lower influence, as that information remains important for our model in future training or with new data (such as a larger dataset).

- Therefore, we can use these techniques to validate our data while also comparing them with qualitative knowledge. It is a complementary approach, not a substitute.

- Another method to validate data is to train a model with different groups of variables: including or excluding a subset of columns and observing the outcomes to select the best model.

### Colineality Analysis
"""

correlation_matrix = df_final.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Matriz de Correlaci√≥n para Analizar Multicolinealidad')
plt.show()

df_vif = df_final.drop(columns=['bwght_kg'])
vif_data = pd.DataFrame()
vif_data["feature"] = df_vif.columns
vif_data["VIF"] = [variance_inflation_factor(df_vif.values, i) for i in range(df_vif.shape[1])]

vif_data

"""## Based on the Variance Inflation Factor (VIF) values provided for each variable, we can draw the following interpretations and conclusions:

### Interpretation of VIF Results:
- Variables with VIF Less than 5: The variables meduc, monpre, npvis, feduc, omaps, fmaps, cigs, drink, male, and smoking have VIF values below 5.
These values show that there's no big issue of multicollinearity among these variables, so they can be seen as independent variables in the model.
Variables with VIF Greater than 5:

- The variables mage and fage have VIF values above 5 (2.19 and 2.54 respectively).
These values suggest there's a moderate level of multicollinearity between these variables, which could impact the accuracy of estimated coefficients in a regression model.
Variables with Infinite VIF:

- The variables mwhte, mblck, moth, fwhte, fblck, and foth have infinite VIF values.
An infinite VIF indicates a perfect multicollinearity among these variables, which could lead to significant issues in the model and affect result interpretation.
Conclusions:
It's a good idea to think about removing or transforming variables with high VIF (above 5) to tackle multicollinearity and enhance the model's stability.
Variables with infinite VIF show perfect multicollinearity and should be handled carefully. They might be considered for exclusion from the model if they don't offer significant information.
It's crucial to make adjustments in the model, like feature selection or using regularization techniques, to deal with multicollinearity effects and improve prediction quality.
In short, addressing the identified multicollinearity in the mage and fage variables is key, along with considering the impact of variables with infinite VIF in the analysis. Making suitable adjustments will help create a stronger and more accurate model.
"""

# Dividir los datos en variables independientes (X) y variable dependiente (y)
X = df_final.drop('bwght_kg', axis=1)
y = df_final['bwght_kg']

# Crear un modelo de Regresi√≥n Lineal
model = LinearRegression()

# Inicializar Recursive Feature Elimination with Cross-Validation (RFE-CV)
rfe = RFECV(estimator=model, cv=5)  # 5-fold cross-validation

# Ajustar RFE al modelo y datos
rfe.fit(X, y)

# Obtener las columnas seleccionadas
selected_columns = X.columns[rfe.support_]

# Crear un DataFrame con las variables seleccionadas
df_selected_features = pd.DataFrame({'Feature': selected_columns})

# Mostrar las variables seleccionadas
df_selected_features

X = df_final[selected_columns]  # We're using a subset of variables to traing our model.
y = df_final['bwght_kg']

# Train, Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestRegressor()

# Hyperparameters for GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200, 300, 400]
}

# (5-fold)Cross VAlidation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# Best model after optimizing the hyperparameters
best_rf = grid_search.best_estimator_

best_rf.fit(X_train, y_train)

y_pred = best_rf.predict(X_test)

# Calcular m√©tricas de evaluaci√≥n del modelo
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Crear un diccionario con las m√©tricas
metrics_dict = {
    'Mean Squared Error': [mse],
    'Mean Absolute Error': [mae],
    'R^2 Score': [r2]
}

# Crear un DataFrame con las m√©tricas
df_metrics = pd.DataFrame(metrics_dict)

# Mostrar las m√©tricas
df_metrics

# Obtener el n√∫mero √≥ptimo de √°rboles seleccionado por GridSearchCV
optimal_n_estimators = best_rf.get_params()['n_estimators']

"""###Based on the evaluation metrics provided for the Random Forest Regressor model with the selected features, we can draw the following conclusions:

### Model Conclusions:
- Mean Squared Error (MSE): The MSE obtained of 0.100179 indicates that, on average, the model predictions have a mean squared error of approximately 0.100179 square units. A lower MSE suggests a better predictive ability of the model.
- Mean Absolute Error (MAE): The MAE of 0.23865 suggests that, on average, the model predictions have a mean absolute error of about 0.23865 units. MAE represents the average magnitude of errors in predictions.
R^2 Score:

- The coefficient of determination R^2 of 0.816088 indicates that around 81.61% of the variability in the target variable is explained by the model. A value of R^2 closer to 1 suggests a good fit of the model to the data.

- Optimal Number of Trees: It was determined that the optimal number of trees for the Random Forest Regressor is 200, indicating that this number provides the best predictive performance in this specific case.
Overall Evaluation:
The model appears to have a strong performance, with an R^2 Score of 0.816088 indicating a good ability to explain variability in the data.
Although the MSE and MAE are not extremely low, they are acceptable values depending on the context of the problem and can be considered satisfactory for the predictions made by the model.

###In summary, the Random Forest Regressor model with 200 trees and the selected features shows a good fit to the data, with a significant ability to predict birth weight (bwght_kg) based on the available features in the dataset.

## Model conclusions with SHAP: global feature importances using SHAP
"""

# Explainer object for SHAP with the best trained Model

explainer = shap.Explainer(best_rf)
shap_values = explainer.shap_values(X)

shap.summary_plot(shap_values, X, plot_type="bar")

"""## Interpreting a SHAP bar plot for feature importances involves understanding the impact of each feature on the model's predictions.

- Feature Importance Ranking: The features are listed on the y-axis of the bar plot, with the most important feature at the top and the least important at the bottom. The length of each bar represents the magnitude of the feature's impact on the model predictions.

- Positive and Negative Impact: The color of the bars indicates whether a feature has a positive or negative impact on the prediction. Blue bars represent features that have a negative impact on the prediction, while red bars represent features with a positive impact.

- Bar Length: The longer the bar for a feature, the more significant its impact on the model predictions. Features with longer bars have a greater influence on the output of the model.

- Direction of Impact: For features with positive impact (red bars), higher values of that feature tend to increase the predicted outcome, while lower values decrease it. Conversely, for features with negative impact (blue bars), higher values decrease the predicted outcome and lower values increase it.

- Overall Importance: By examining the overall distribution of bar lengths and colors, you can determine which features are the most influential in driving predictions and understand their relative importance in the model.

- Interactions: Additionally, SHAP values can help you understand interactions between features. If certain features have similar impacts on predictions, their effects may be correlated or interact with each other in the model.

#### In summary, when interpreting a SHAP bar plot for feature importances, focus on the ranking, direction, and magnitude of each feature's impact on the model predictions to gain insights into how different variables contribute to the model's decision-making process.
"""

# Definir el modelo Random Forest Regressor
rf = RandomForestRegressor()

# Realizar la validaci√≥n cruzada con 5 folds
scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')

# Convertir los resultados de MSE a positivos y calcular el promedio
mse_scores = -scores
mean_mse = mse_scores.mean()

# Mostrar los resultados de la validaci√≥n cruzada
print("Mean Squared Error for each fold:", mse_scores)
print("Mean Squared Error (Mean):", mean_mse)

# Realizar la validaci√≥n cruzada con 5 folds y obtener los resultados de MSE
scores = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')
mse_scores = -scores

# Crear un gr√°fico de barras para mostrar los resultados de MSE en cada fold
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(mse_scores)+1), mse_scores, color='skyblue')
plt.xlabel('Fold')
plt.ylabel('Mean Squared Error')
plt.title('Mean Squared Error for Each Fold in Cross-Validation')
plt.xticks(range(1, len(mse_scores)+1))
plt.show()

"""### The provided text actually shows the calculation of the Mean Squared Error (MSE) for each fold of cross-validation in a Random Forest Regressor model.

### Interpretation of the Bar Graph:
- X-Axis: Each bar in the graph represents a specific fold (a fold is a subset or data partition in the train & test subsets) of cross-validation, numbered from 1 to 5 in this case.
- Y-Axis: The height of each bar indicates the value of the Mean Squared Error (MSE) obtained in that particular fold.
Color of Bars: In the example, the bars are colored in light blue, but the color can vary according to preference.
### General Interpretation:
- A shorter bar indicates a lower MSE, meaning that the model performed better in that specific fold.
- Variability: The variability in the heights of the bars shows how the model's performance may fluctuate across different data partitions during cross-validation.
- Consistency: If all bars are similar in height, it indicates that the model has consistency in its performance across different folds.

### Summary:
- Overall, when interpreting a bar graph for cross-validation, you look for consistency in the model's performance across different folds and observe any significant variability that may indicate the model's sensitivity to data partitioning.

"""

